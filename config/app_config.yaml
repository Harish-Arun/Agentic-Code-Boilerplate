# ============================================================
# NNP-AI Application Configuration (Infrastructure / Tech)
# ============================================================
# This file contains ONLY infrastructure, service, and
# technical settings. No business rules live here.
#
# Business logic (thresholds, prompts, decision rules) â†’
#   config/business_config.yaml  (requires business sign-off)
#
# Owner: Engineering / DevOps
# ============================================================

# ============================================
# Service Ports
# ============================================
services:
  api_gateway:
    host: "0.0.0.0"
    port: 8000
  agents:
    host: "0.0.0.0"
    port: 8001
  mcp_tools:
    host: "0.0.0.0"
    port: 8002

# ============================================
# Database Configuration (Pluggable)
# ============================================
database:
  type: "sqlite"  # Options: sqlite, postgres, mongo, dynamodb

  sqlite:
    path: "${DATA_DIR:-./data}/nnp_ai.db"

  postgres:
    host: "${POSTGRES_HOST:-localhost}"
    port: ${POSTGRES_PORT:-5432}
    database: "${POSTGRES_DB:-nnp_ai}"
    user: "${POSTGRES_USER:-admin}"
    password: "${POSTGRES_PASSWORD:-}"

  mongo:
    uri: "${MONGO_URI:-mongodb://localhost:27017/nnp_ai}"

  dynamodb:
    region: "${AWS_REGION:-eu-west-2}"
    table_prefix: "${DYNAMO_TABLE_PREFIX:-nnp_}"
    endpoint_url: "${DYNAMO_ENDPOINT:-}"  # For local DynamoDB

# ============================================
# LLM Configuration
# ============================================
llm:
  provider: "${LLM_PROVIDER:-gemini}"  # Options: gemini, openai, azure

  gemini:
    model: "${GEMINI_MODEL:-gemini-3-flash-preview}"
    temperature: 0.0
    max_tokens: null  # null = unlimited
    top_k: 1
    top_p: 0.1
    candidate_count: 1
    # Thinking configuration
    thinking_budget: -1  # -1=dynamic, 0=off, >0=specific tokens (Gemini 2.5)
    thinking_level: null  # minimal, low, medium, high (Gemini 3)
    include_thoughts: false  # Include thought summaries in response

  openai:
    model: "gpt-4"
    temperature: 0.1

  azure:
    endpoint: "${AZURE_OPENAI_ENDPOINT:-}"
    deployment: "${AZURE_OPENAI_DEPLOYMENT:-}"
    api_version: "2024-02-15-preview"

# ============================================
# Agent Orchestrator Configuration
# ============================================
agents:
  enabled:
    - extraction
    - signature_detection
    - verification

  retry:
    max_attempts: 3
    backoff_seconds: 1

  checkpointing:
    enabled: true
    backend: "memory"  # Options: memory, redis, postgres

# ============================================
# MCP Tools Server Configuration
# ============================================
mcp:
  server:
    host: "0.0.0.0"
    port: 8002
    transport: "sse"  # Options: sse, http, stdio

  tools:
    enabled:
      - extraction
      - signature_detection
      - signature_verification
      - ocr
      - pdf_utils
      - signature_provider

    pdf_utils:
      max_pages: 50
      dpi: 300

    signature_provider:
      cache_enabled: true
      cache_ttl_seconds: 3600

# ============================================
# Feature Flags
# ============================================
features:
  human_in_loop: false  # Set true for production (Stage 4)
  signature_verification: true
  auto_extraction: true
  audit_logging: true
  retry_on_failure: true

# ============================================
# Logging
# ============================================
logging:
  level: "${LOG_LEVEL:-INFO}"
  format: "json"  # Options: json, text
  include_timestamp: true

# ============================================
# File Storage Paths
# ============================================
storage:
  data_dir: "${DATA_DIR:-./data}"
  uploads_dir: "${DATA_DIR:-./data}/uploads"
  signatures_dir: "${DATA_DIR:-./data}/signatures"
  reference_dir: "${DATA_DIR:-./data}/reference"
  debug_dir: "${DATA_DIR:-./data}/debug"

# ============================================
# External Document Sources (Stage 1 - Ingestion)
# ============================================
ingestion:
  sources:
    manual_upload:
      enabled: true
    network_drive:
      enabled: false
      protocol: "smb"          # smb, nfs, cifs
      path: "${NETWORK_DRIVE_PATH:-}"
      poll_interval_seconds: 60
    s3_bucket:
      enabled: false
      bucket: "${S3_BUCKET:-}"
      prefix: "${S3_PREFIX:-incoming/}"
      region: "${AWS_REGION:-eu-west-2}"
      poll_interval_seconds: 30
